
How it feels when you understand Why and How at the same time. The goal of this blog post is to understand the working of Pytorch Autograd module by understanding the tensor functions related to it.

There are some prerequisites though :

1. Understanding of basics calculus like chain rule.
2. Understanding of basics of Deep Learning like Backpropagation.
3. Basic familiarity with PyTorch's tensors. ( A very quick revision for PyTorch tensors: here)

Let's jump into the blog.

# Autograd
It is a tool that does the calculation of derivatives via a technique called automatic differentiation. As quoted from the official documentation: torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar-valued functions.
Automatic differentiation is a set of techniques to numerically evaluate the derivative of a function. As it is required during the backpropagation pass(to compute the gradient of weights w.r.t loss function) while training a neural network.

# Computation Graph
So how does during backpropagation, PyTorch(or any other DL library for that matter) calculates gradients, it does by generating a data structure called Computation graph. In a complex setup where there are thousands of variables to calculate the gradient, a computation graph comes into the picture.
Computation graph is nothing but a simple map of references of variables(or tensors) and operators(or functions) generated for a set of algebraic equations, through which autograd can traverse and trace back (to leaves) to calculate gradients.

Now, as PyTorch generate these graphs during runtime in a forward pass(simple calculation of outputs from inputs), graphs are called Dynamic Computation Graphs.

Example 1: Consider the following set of equations, during a forward pass the following DCG will be built.
```
x = torch.tensor(3.)
a = torch.tensor(4.)
b = torch.tensor(5.)
y = a * x 
z = y + b
```

![](https://miro.medium.com/max/839/1*zCI8vO39EtV26L3zp56qJA.png)

DCG if `require_grad=False` 

As you can notice this doesn't include a backward pass anywhere. Yes, It doesn't! and that's because we do not ask it to draw one.
So buckle up we will go through about 5 functions of tensors to find out in the end how and what will be the computation graph for the backward pass.

## 1. Important properties : requires_grad , grad_fn , is_leaf
### 1.1. requires_grad
The requires_grad attribute tells autograd to track your operations. So if you want PyTorch to create a graph corresponding to these operations, you will have to set the requires_grad attribute of the Tensor to True.
There are 2 ways in which it can be done, either by passing it as an argument in torch.tensor or explicitly setting up the requires_grad property to True.
It is to remember that tensors with only float data types can require gradient (or ask autograd to record its operations).

**Points to ponder :**

The Tensors generated by applying any operations on other tensors, given that the for at least one input tensor requires_grad = True, then the resultant tensor will also have requires_grad = True.

It is also helpful when in a network we don't want to change the gradients and hence don't want to update the weights associated with some tensors. Just setting require_grad = False the tensors won't participate in the computation graph.

### 1.2. grad_fn
The grad_fn property holds the reference to the function (mathematical operator) that creates it. It is very important during a backward pass as the function here is responsible to calculate the gradient and send it to the appropriate next function in the next pass.
If requires_grad is set to False, grad_fn would be None.

### 1.3. is_leaf
The is_leaf property tells whether a tensor is a leaf node or not. Essentially leaf tensors are the tensors whom we want to accumulate the gradient and are present at the edge of the computation graph. Only leaf Tensors will have their grad populated during a call to backward(). Technically, the leaf tensors are any tensors that created by the following approaches :
Tensors resulting in operations from tensors that have `requires_grad = False` will be leaf Tensors.
Any tensor that is explicitly created by the user will be leaf Tensors. This means as they are not the result of an operation and so grad_fn = None.

In the following example, the tensor x is only the leaf node. And as x is a leaf node, the grad_fn = None (as it is not obtained from any operations).
The tensor y has grad_fn a multiplication operator since y is obtained from the multiplication of a and x. Similarly the case for z.

## 2. backward()
The signature for backward is backward(gradient=None, retain_graph=None, create_graph=False) .
This the most important of the tensor methods present here. It computes the gradient of current tensor w.r.t. graph leaves. It is responsible to calculate the gradient during a backward pass.

1. These are the typical steps involved in gradient calculation during a backward pass :

2. The backward function takes an incoming gradient from the part of the network in front of it.

3. Then it calculates the local gradient at a particular tensor.

4. Then it multiplies the local gradient to with incoming gradient.

5. Finally, forwards the computed gradient to the tensor's inputs by invoking the backward method of the grad_fn of their inputs or simply save the gradient in grad property for leaf nodes.

Suppose in the above example, when calling z.backward. The grad_fn of z is <AddBackward>.
The backward function of <AddBackward> takes a default input tensor as torch.tensor([1.]).
Then it calculates gradient for y and b. For both, the gradients will be [1.] as the operator is an addition function.
The gradient is multiplied with the incoming tensor i.e. [1.] * [1.].
Now for b, the grad_fn = None so the gradient computed directly will get stored in grad property of tensor b. And for tensor y the backward function passes the gradient to its input tensor's grad_fn (i.e. <MulBackward> of y since it is formed after the multiplication of x and a)
Similarly, the backward function will be called for y's <MulBackward> with an input gradient from <AddBackward> i.e. [1.] in this case.

As noticed, the backward function is recursively called throughout the graph as we backtrack. You can access the gradients by calling the grad attribute of Tensor.
Note: backward function only calculates gradients by going over an already made backward graph. The backward graph is as discussed generated during a forward pass only.
  
### 2.1. Calling backward() on non-scaler tensor
  For a vector-valued tensor, the backward function gives a Runtime error: grad can be implicitly created only for scalar outputs.
  This is because for a non-scalar tensor a jacobian-vector is to be computed and then the backward expects incoming gradients as it's input (usually the gradient of the
  differentiated function w.r.t. corresponding tensors). Hence the backward expects incoming gradient a Tensor of the same size as the current tensor, then it'll able to
  backpropagate.

  **So either you can pass the tensor of the same shape,**
  ```
  x = torch.tensor(3., requires_grad=True)
  a = torch.tensor([4.,2.], requires_grad=True)
  z = x + a

  z.backward(torch.tensor([1.,1.])) # passing a gradient to backward.
  ```

  **Or simply change the size of the current tensor to torch.Size([]) as expected by backward.**

 
  ```
  x = torch.tensor(3., requires_grad=True)
  a = torch.tensor([4.,2.], requires_grad=True)
  z = x + a
  z = z.mean()
  z.backward()
  ```

  Note : If you'll pass non-ones tensor in backward, the gradients will get scaled accordingly.

---

  **Wow, now we have understood the basic functioning of Autograd in PyTorch along with functions to implement that. But we'll wait now and get back to our Computation Graph
  diagram for the same equation to concretize the concept.**

  Following is the DCG of the same Example (Example 1) when we have require_grad = True

  1. The tensors in green are leaf nodes.

  2. Tensors in yellow are intermediate nodes.

  3. MulBackward and AddBackward are two grad_fn for y and z respectively.

  4. grad attribute stores the value of calculated gradients.

  ![](https://miro.medium.com/max/860/1*tfVJSuDLRtMXv_kCdB4qHA.png)

  DCG if require_grad=True

---

## 3. retain_grad()
  This function allows a tensor that is not a leaf node to store the gradient that passes through it during the backward pass. It can help to troubleshoot the flow of gradients
  in the graph.

## 4. register_hook()
The hook will be called every time a gradient(w.r.t a Tensor) is computed. The hook should have the following signature:
hook(grad) -> Tensor or None
 
So, the hook can take the value of grad and can return a new value or perform operations with the value.
This is the best part, this can help to
Modify the grad on the fly during a backward pass without waiting for the pass to be completed. This can influence our ways to calculate the gradient in a graph.
Debug the code for the flow of gradients in your graph. Identifying gradients at each step even for non-leaf nodes.

Looking at an example from the PyTorch documentation.

## 5. detach()
It creates a copy of a tensor that is not a part of the computation graph i.e detaches the Tensor from the graph that created it, making it a leaf.
Both tensors will share the same memory, output tensor will be a leaf with grad_fn = None and requires_grad = False.

Conclusion
We have discussed the most important tensor functions in the context of Autograd in Pytorch. This will help in building a solid foundation about the working of Pytorch Autograd module in general.
Link to the associated jupyter notebook on GitHub.
Thank you for reading. Do mention your queries, responses or feedback.
